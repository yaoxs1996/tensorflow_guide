{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bityaoconda454bf62b1e524940b398a92050af6e17",
   "display_name": "Python 3.7.7 64-bit ('yao': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Keras中的遮盖和填充"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "source": [
    "__遮盖__：告知序列处理层输入中有些时间步骤丢失，因此在处理数据时应将其跳过\n",
    "\n",
    "__填充__：遮盖的特殊形式，其中被遮盖的步骤位于序列的起点或开头。填充是出于将序列数据编码成连续批次的需要：为了使批次中的所有序列适合给定的标准长度，有必要填充或截断某些序列。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 填充序列数据\n",
    "\n",
    "例如数据是嵌套列表，各个样本的长度不一。但深度学习模型的输入数据必须是单一张量，短于最长条目的样本需要用占位符值进行填充，或者在填充短样本前截断长样本。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 711  632   71    0    0    0]\n [  73    9 3215   55  927    0]\n [  83   91    1  645 1253  927]]\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    [711, 632, 71],\n",
    "    [73, 9, 3215, 55, 927],\n",
    "    [83, 91, 1, 645, 1253, 927]\n",
    "]\n",
    "\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs, padding=\"post\")\n",
    "print(padded_inputs)"
   ]
  },
  {
   "source": [
    "## 遮盖\n",
    "\n",
    "经过填充后，需要告知模型，哪些部分实际上是填充的，应该忽略。这种机制就是遮盖。\n",
    "\n",
    "Keras中遮盖的三种方式：\n",
    "\n",
    "* 添加一个`keras.layers.Masking`层\n",
    "* 使用`mask_zero=True`配置一个`keras.layers.Embedding`层\n",
    "* 在调用支持`mask`参数的层时，手动传递参数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 掩码生成层：Embedding和Masking\n",
    "\n",
    "在后台创建一个掩码张量（`(batch, sequence_length)`的二维张量），将其附加到由`Masking`或`Embedding`层返回的张量输出上。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True False False False]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[ True  True  True False False False]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "masked_output = embedding(padded_inputs)\n",
    "\n",
    "print(masked_output._keras_mask)\n",
    "\n",
    "masking_layer = layers.Masking()\n",
    "\n",
    "unmasked_embedding = tf.cast(tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1,1,10]), tf.float32)\n",
    "\n",
    "masked_embedding = masking_layer(unmasked_embedding)\n",
    "print(masked_embedding._keras_mask)"
   ]
  },
  {
   "source": [
    "## 函数式API和序列式API中的掩码传播"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列模型\n",
    "model = keras.Sequential(\n",
    "    [layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True), layers.LSTM(32)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数式API\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
    "outputs = layers.LSTM(32)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "source": [
    "## 将掩码张量直接传递给层\n",
    "\n",
    "能够处理掩码的层，如`LSTM`层，在其`__call__`方法中有一个`mask`参数。\n",
    "\n",
    "在生成掩码的层，如`Embedding`，会公开一个`compute_mask(input, previous_mask)`方法。\n",
    "\n",
    "可以将掩码生成层的`compute_mask()`方法的输出传递给掩码使用层的`__call__`方法。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[ 0.00654859,  0.0051147 ,  0.00080724, ..., -0.00455279,\n",
       "         0.0029917 ,  0.0041261 ],\n",
       "       [ 0.01040531, -0.00743341,  0.01023701, ...,  0.00405205,\n",
       "        -0.00841662, -0.00174078],\n",
       "       [ 0.00652924,  0.00207402,  0.00406809, ..., -0.0088197 ,\n",
       "         0.00276266, -0.01376904],\n",
       "       ...,\n",
       "       [-0.01551128,  0.00162226,  0.00389638, ...,  0.00463707,\n",
       "        -0.00219373, -0.0003141 ],\n",
       "       [ 0.00514781,  0.00777179,  0.00195215, ..., -0.00056386,\n",
       "        -0.00011741, -0.0039389 ],\n",
       "       [ 0.00282336,  0.00047061,  0.00593017, ..., -0.00324067,\n",
       "         0.00156855,  0.00412254]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "        self.lstm = layers.LSTM(32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        # mask tensor可以手动准备，但是请注意其形状\n",
    "        mask = self.embedding.compute_mask(inputs)\n",
    "        output = self.lstm(x, mask=mask)\n",
    "        return output\n",
    "\n",
    "layer = MyLayer()\n",
    "x = np.random.random((32, 10)) * 100\n",
    "x = x.astype(\"int32\")\n",
    "layer(x)"
   ]
  },
  {
   "source": [
    "## 在自定义层中支持遮盖\n",
    "\n",
    "任何生成与其具有不同时间维度的张量的层，都需要修改当前的掩码，这样下游层才能正确估计被遮盖的时间步骤。\n",
    "\n",
    "自定义层需要实现`layer.compute_mask()`方法，还方法依据输入和当前掩码生成新的掩码。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n[[ True  True  True]\n [ True  True  True]\n [ True  True  True]], shape=(3, 3), dtype=bool)\ntf.Tensor(\n[[False False False]\n [ True  True False]\n [ True  True  True]], shape=(3, 3), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "class TemporalSplit(layers.Layer):\n",
    "    # 沿时间维度，将输入张量划分成两个张量\n",
    "    def call(self, inputs):\n",
    "        return tf.split(inputs, 2, axis=1)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        return tf.split(mask, 2, axis=1)\n",
    "\n",
    "first_half, second_half = TemporalSplit()(masked_embedding)\n",
    "print(first_half._keras_mask)\n",
    "print(second_half._keras_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n",
    "        super(CustomEmbedding, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.mask_zero = mask_zero\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self.input_dim, self.output_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
    "\n",
    "    def compute_mask(self, )"
   ]
  }
 ]
}